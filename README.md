# CNN vs ViT: ë°ì´í„° ê·œëª¨ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ ì—°êµ¬
Data-Scale Sensitivity Analysis of CNN (ResNet-18) and ViT (Tiny-Patch16)

[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=flat-square&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Colab](https://img.shields.io/badge/Google_Colab-F9AB00?style=flat-square&logo=googlecolab&logoColor=white)](https://colab.research.google.com/)
[![Apple Silicon](https://img.shields.io/badge/Apple_Silicon-MPS-000000?style=flat-square&logo=apple&logoColor=white)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“ ì—°êµ¬ ê°œìš”

ì´ ì—°êµ¬ëŠ” ë°ì´í„° ê·œëª¨(Data Scale)ê°€ CNN(ResNet-18)ê³¼ Vision Transformer(ViT-Tiny)ì˜ ëª¨ë¸ ì„±ëŠ¥ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ë¹„êµÂ·ë¶„ì„í•˜ê¸° ìœ„í•´ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.

íŠ¹íˆ, CNNì´ ê°€ì§„ inductive biasì™€ ViTì˜ ë°ì´í„° ì˜ì¡´ì„±ì´ ì‹¤ì œ ì‹¤í—˜ì—ì„œ ì–´ë–¤ í˜•íƒœë¡œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ê²€ì¦í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

---

## 1. ì—°êµ¬ ë°°ê²½

### 1.1 CNN 
- Convolution(í•„í„°)ì„ ì‚¬ìš©í•´ ì§€ì—­ì  íŠ¹ì§•ì„ íƒìƒ‰
- ì´ë¯¸ì§€ êµ¬ì¡°ì— íŠ¹í™”ëœ inductive bias ë‚´ì¥
- ì ì€ ë°ì´í„°ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ ì„±ëŠ¥ í™•ë³´
  
### 1.2 Vision Transformer
- ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜(patch) ë‹¨ìœ„ë¡œ ë¶„í• í•´ í† í°ì²˜ëŸ¼ ì²˜ë¦¬
- Self-Attentionìœ¼ë¡œ ì „ì—­ ê´€ê³„ë¥¼ í•™ìŠµ
- ì´ë¯¸ì§€ êµ¬ì¡°ì— ëŒ€í•œ ì„ ì²œì  ê°€ì •ì´ ê±°ì˜ ì—†ìŒ â†’ ë§ì€ ë°ì´í„° í•„ìš”

### 1.3 ì—°êµ¬ ì§ˆë¬¸
- ë°ì´í„° ë¹„ìœ¨(10%, 25%, 50%, 100%) ë³€í™”ê°€ ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ì–´ë–¤ ì°¨ì´ë¥¼ ë§Œë“œëŠ”ê°€?
- ì‘ì€ ë°ì´í„° ìƒí™©ì—ì„œ CNNì´ ë” ê°•ë ¥í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?
- ViTëŠ” ì–´ëŠ ì‹œì ì—ì„œ CNNê³¼ ì„±ëŠ¥ ê²©ì°¨ê°€ ì¤„ì–´ë“œëŠ”ê°€?

---

## 2. Experimental Methodology

### 2.1 Model Architectures

We compared two representative models with similar parameter scales:

#### CNN
- **Model:** ResNet-18 (He et al.)
- **Characteristics:** Strong inductive bias (Locality, Translation Invariance)
- **Optimization:** Modified first conv layer (kernel 3Ã—3) for CIFAR-10 (32Ã—32)

#### Vision Transformer
- **Model:** vit_tiny_patch16_224 (Dosovitskiy et al.)
- **Characteristics:** Long-range dependency modeling via Self-Attention, low inductive bias
- **Preprocessing:** Resize images to 224Ã—224

### 2.2 Computing Environment (Hybrid Strategy)

| Architecture | Hardware | Framework | Script |
|:-------------|:---------|:----------|:-------|
| **CNN** | **MacBook Air M3** (16GB) | PyTorch (MPS) | `train_cnn.py` |
| **ViT** | **Google Colab TPU** (v2) | PyTorch XLA | `train_vit.py` |

---

## 3. Directory Structure

```bash
CNN-vs-ViT/
â”œâ”€â”€ README.md               # Project Report 
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ train_cnn.py            # Script for Mac M3 (MPS)
â””â”€â”€ train_vit.py            # Script for Cloud TPU (XLA)
```

---

## 4. Usage

### 4.1 Installation

```bash
git clone https://github.com/rokcpla2/CNN-vs-ViT.git
cd CNN-vs-ViT
pip install -r requirements.txt
```

### 4.2 Train CNN (Local Edge Device)

Optimized for Apple Silicon (MPS). You can control the data ratio using the `--ratio` argument.

```bash
# Train with 10% data (Fast experiment)
python train_cnn.py --ratio 0.1 --epochs 50

# Train with 100% data (Full experiment)
python train_cnn.py --ratio 1.0 --epochs 50
```

### 4.3 Train ViT (Cloud TPU)

Optimized for TPU environments (e.g., Google Colab).

```bash
# Train with 25% data
python train_vit.py --ratio 0.25 --epochs 50
```

---

## 5. Experimental Results ğŸ“Š

### 5.1 Performance Comparison Graph

<p align="center">
  <img src="https://github.com/user-attachments/assets/18137eea-70eb-4908-92b0-5c636110ddbb" width="845" height="573" alt="final_result_dark">
</p>

The graph below illustrates the Test Accuracy trends as the dataset size increases.

### 5.2 Numerical Analysis

| Data Ratio | CNN (ResNet-18) | ViT (Tiny) | Performance Gap |
|:-----------|:----------------|:-----------|:----------------|
| 10% (5k) | 63.40% | 45.01% | +18.39% |
| 25% (12.5k) | 72.01% | 55.30% | +16.71% |
| 50% (25k) | 79.13% | 65.24% | +13.89% |
| 100% (50k) | 82.23% | 73.33% | +8.90% |

### 5.3 Key Findings

#### Inductive Bias Matters in Low Data
CNN consistently outperformed ViT, especially when data was scarce (10%), thanks to its architectural priors.

#### ViT is "Data Hungry"
ViT showed a steeper learning curve. The performance gap closed from 18.4% (at 10% data) to 8.9% (at 100% data).

#### Overfitting
At 100% data, ViT achieved 95.8% Training Acc but only 73.3% Test Acc, indicating a need for stronger regularization (e.g., Mixup, CutMix) or more data.

---

## 6. Conclusion

This study empirically validates that CNNs are more data-efficient and suitable for edge-based scenarios with limited resources. Conversely, ViTs show higher scalability but require significantly more data to overcome the lack of inductive bias.

### Future Work

We plan to develop an **Adaptive Edge-Cloud Inference System** that dynamically offloads difficult samples to a cloud-based ViT while processing easy samples on a local CNN, leveraging the hybrid environment established in this project.

---

## ğŸ‘¨â€ğŸ’» Author

**Minkyu Kim**  
Dept. of Electronic Engineering, KNUT  
Research Interest: Embedded AI, FPGA Acceleration, Computer Vision

---

## ğŸ“„ License

This project is licensed under the MIT License.
