# CNN vs ViT: ë°ì´í„° ê·œëª¨ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ ì—°êµ¬
### Data-Scale Sensitivity Analysis of CNN (ResNet-18) and ViT (Tiny-Patch16)

[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=flat-square&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Colab](https://img.shields.io/badge/Google_Colab-F9AB00?style=flat-square&logo=googlecolab&logoColor=white)](https://colab.research.google.com/)
[![Apple Silicon](https://img.shields.io/badge/Apple_Silicon-MPS-000000?style=flat-square&logo=apple&logoColor=white)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“ ì—°êµ¬ ê°œìš”

ì´ ì—°êµ¬ëŠ” ë°ì´í„° ê·œëª¨(Data Scale)ê°€ CNN(ResNet-18)ê³¼ Vision Transformer(ViT-Tiny)ì˜ ëª¨ë¸ ì„±ëŠ¥ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ë¹„êµÂ·ë¶„ì„í•˜ê¸° ìœ„í•´ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.

íŠ¹íˆ, CNNì´ ê°€ì§„ inductive biasì™€ ViTì˜ ë°ì´í„° ì˜ì¡´ì„±ì´ ì‹¤ì œ ì‹¤í—˜ì—ì„œ ì–´ë–¤ í˜•íƒœë¡œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ê²€ì¦í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

---

## 1. ì—°êµ¬ ë°°ê²½

### 1.1 CNN 
- Convolution(í•„í„°)ì„ ì‚¬ìš©í•´ ì§€ì—­ì  íŠ¹ì§•ì„ íƒìƒ‰
- ì´ë¯¸ì§€ êµ¬ì¡°ì— íŠ¹í™”ëœ inductive bias ë‚´ì¥
- ì ì€ ë°ì´í„°ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ ì„±ëŠ¥ í™•ë³´
  
### 1.2 Vision Transformer
- ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜(patch) ë‹¨ìœ„ë¡œ ë¶„í• í•´ í† í°ì²˜ëŸ¼ ì²˜ë¦¬
- Self-Attentionìœ¼ë¡œ ì „ì—­ ê´€ê³„ë¥¼ í•™ìŠµ
- ì´ë¯¸ì§€ êµ¬ì¡°ì— ëŒ€í•œ ì„ ì²œì  ê°€ì •ì´ ê±°ì˜ ì—†ìŒ â†’ ë§ì€ ë°ì´í„° í•„ìš”

### 1.3 ì—°êµ¬ ì§ˆë¬¸
- ë°ì´í„° ë¹„ìœ¨(10%, 25%, 50%, 100%) ë³€í™”ê°€ ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ì–´ë–¤ ì°¨ì´ë¥¼ ë§Œë“œëŠ”ê°€?
- ì‘ì€ ë°ì´í„° ìƒí™©ì—ì„œ CNNì´ ë” ê°•ë ¥í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?
- ViTëŠ” ì–´ëŠ ì‹œì ì—ì„œ CNNê³¼ ì„±ëŠ¥ ê²©ì°¨ê°€ ì¤„ì–´ë“œëŠ”ê°€?

---

## 2. ì‹¤í—˜ í™˜ê²½

### 2.1 ë°ì´í„°ì…‹
- CIFAR-10 (32Ã—32 RGB, 10 classes)
- ì‚¬ìš© ë¹„ìœ¨: 10%, 25%, 50%, 100%

### 2.2 ëª¨ë¸
CNN: ResNet-18
- CIFAR-10ì— ë§ê²Œ 3Ã—3 Convë¡œ ìˆ˜ì •

ViT: ViT-Tiny-Patch16
224Ã—224 Resize í›„ Patch-Embedding
Self-Attention ê¸°ë°˜ êµ¬ì¡°


### 2.3 ì»´í“¨íŒ… í™˜ê²½

| Architecture | Hardware | Framework | Script |
|:-------------|:---------|:----------|:-------|
| CNN | MacBook Air M3 (16GB) | PyTorch | `train_cnn.py` |
| ViT | Google Colab TPU | PyTorch XLA | `train_vit.py` |

---

## 3. ì½”ë“œ êµ¬ì¡°

```bash
CNN-vs-ViT/
â”œâ”€â”€ README.md               # Project Report 
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ train_cnn.py            # Script for Mac M3 (MPS)
â””â”€â”€ train_vit.py            # Script for Cloud TPU (XLA)
```

---

## 4. ì‹¤í–‰ ë°©ë²•

### 4.1 ì„¤ì¹˜

```bash
git clone https://github.com/rokcpla2/CNN-vs-ViT.git
cd CNN-vs-ViT pip install -r requirements.txt
```

### 4.2 CNN í•™ìŠµ (Local Edge Device)

Optimized for Apple Silicon (MPS). You can control the data ratio using the `--ratio` argument.

```bash
# Train with 10% data (Fast experiment)
python train_cnn.py --ratio 0.1 --epochs 50

# Train with 100% data (Full experiment)
python train_cnn.py --ratio 1.0 --epochs 50
```

### 4.3 ViT í•™ìŠµ (Cloud TPU)

Optimized for TPU environments (e.g., Google Colab).

```bash
# Train with 25% data
python train_vit.py --ratio 0.25 --epochs 50
```

---

## 5. ì‹¤í—˜ ê²°ê³¼ ğŸ“Š

### 5.1 Accuracy ë¹„êµ ê·¸ë˜í”„

<p align="center">
  <img src="https://github.com/user-attachments/assets/18137eea-70eb-4908-92b0-5c636110ddbb" width="845" height="573" alt="final_result_dark">
</p>

### ìˆ˜ì¹˜ ë¹„êµ

| Data Ratio | CNN | ViT | Performance Gap |
|:-----------|:----------------|:-----------|:----------------|
| 10% (5k) | 63.40% | 45.01% | +18.39% |
| 25% (12.5k) | 72.01% | 55.30% | +16.71% |
| 50% (25k) | 79.13% | 65.24% | +13.89% |
| 100% (50k) | 82.23% | 73.33% | +8.90% |

## 6. ë¶„ì„ (Analysis)

### 6.1 CNNì€ ì™œ ë°ì´í„°ê°€ ì ì–´ë„ ê°•í•œê°€?
- ì§€ì—­ì  íŒ¨í„´ì„ ìš°ì„ ì ìœ¼ë¡œ ë³´ëŠ” inductive bias
- í•„í„°ê°€ ì „ì²´ ì´ë¯¸ì§€ì— ê³µìœ ë¨
- í•™ìŠµí•´ì•¼ í•  íŒŒë¼ë¯¸í„° ê³µê°„ì´ ìƒëŒ€ì ìœ¼ë¡œ ì¢ìŒ

### 6.2 ViTëŠ” ì™œ ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•œê°€?
- íŒ¨ì¹˜ ê°„ ê´€ê³„ë¥¼ ì „ë¶€ í•™ìŠµí•´ì•¼ í•¨
- ì´ë¯¸ì§€ êµ¬ì¡°ì— ëŒ€í•œ ì‚¬ì „ ê°€ì •ì´ ì—†ìŒ
- ì‘ì€ ë°ì´í„°ì—ì„œëŠ” ì‰½ê²Œ overfitting ë°œìƒ

### 6.3 ì„±ëŠ¥ ê²©ì°¨ê°€ ì¤„ì–´ë“œëŠ” ì§€ì 
- ë°ì´í„°ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ViTì˜ ì¥ì (ì „ì—­ì  íŠ¹ì§• í•™ìŠµ)ì´ ë°œíœ˜ë¨
- 100% ë°ì´í„° êµ¬ê°„ì—ì„œëŠ” Gapì´ ì•½ 8.9%ê¹Œì§€ ê°ì†Œ

---

## 6. ê²°ë¡ (Conclusion)

- ì‘ì€ ë°ì´í„°ì—ì„œëŠ” CNNì´ ì••ë„ì ìœ¼ë¡œ ìœ ë¦¬
- ViTëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ì ì°¨ CNNê³¼ ê²©ì°¨ë¥¼ ì¢í˜
- ëª¨ë¸ ì„ íƒì€ ë°ì´í„° ì¡°ê±´ì— ë”°ë¼ ë‹¬ë¼ì ¸ì•¼ í•¨

### í–¥í›„ ì—°êµ¬ ë°©í–¥ (Future Work)

- Mixup/CutMix ê¸°ë°˜ ViT regularization ì‹¤í—˜
- augmentation ì˜í–¥ ë¹„êµ
- patch size ë³€í™”(ablation study)
- CNN-ViT í˜‘ë ¥ êµ¬ì¡°(Edge-Cloud Hybrid Inference) ì—°êµ¬

---

## ğŸ‘¨â€ğŸ’» Author

**Minkyu Kim**  
Dept. of Electronic Engineering, KNUT  
Research Interest: Embedded AI, FPGA Acceleration, Computer Vision

---

## ğŸ“„ License

This project is licensed under the MIT License.
